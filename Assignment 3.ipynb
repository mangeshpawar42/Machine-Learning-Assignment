{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOq4gq0eeSlU+d7mBnYffgk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Spelling Correct in input sentences with help of Dataset of various sentences in English"],"metadata":{"id":"VLzugi4m0XHp"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kA1RXO_1yOR0","executionInfo":{"status":"ok","timestamp":1682667536086,"user_tz":-330,"elapsed":1280,"user":{"displayName":"TY_B_10_Jayendra Borse","userId":"08845089155460103684"}},"outputId":"10f3994d-89be-4e36-da1a-8e3e0057e431"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Turn off the lights ?\n","Open the garage door .\n","Can your please open the door .\n","Turn off the lights in the dining room .\n","lights\n"]}],"source":["import os\n","from nltk import word_tokenize\n","import itertools\n","import pandas as pd\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","df = pd.read_csv(\"dataset.csv\")\n","sentences_df = df[['Sentence']]\n","sentences_df.head(10)\n","\n","\n","def get_plain_vocabluary():\n","    sentencess = [word_tokenize(sentence['Sentence']) for index, sentence in sentences_df.iterrows()]\n","    mergesentences = list(itertools.chain.from_iterable(sentencess))\n","    plainvocabulary = list(set(mergesentences))\n","    return plainvocabulary\n","\n","def levenshtein_distance(s1, s2):\n","    if len(s1) > len(s2):\n","        s1, s2 = s2, s1\n","\n","    distances = range(len(s1) + 1)\n","    for i2, c2 in enumerate(s2):\n","        distances_ = [i2+1]\n","        for i1, c1 in enumerate(s1):\n","            if c1 == c2:\n","                distances_.append(distances[i1])\n","            else:\n","                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n","        distances = distances_\n","    return distances[-1]\n","\n","def spelling_correction(sentence):\n","    splittedsentence = word_tokenize(sentence)\n","    vocwords = list(itertools.chain.from_iterable([get_plain_vocabluary()]))\n","    for i,word in enumerate(splittedsentence):\n","        if (word not in vocwords and not word.isdigit()): # ignore digits\n","            levdistances = []\n","            for vocword in vocwords:\n","                levdistances.append(levenshtein_distance(word,vocword))\n","            splittedsentence[i] = vocwords[levdistances.index(min(levdistances))]\n","        else:\n","            splittedsentence[i] = word\n","    return ' '.join(splittedsentence)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","# 1 word is not spelled correctly \"lihgts\"\n","print(spelling_correction(\"Turn off the lihgts?\"))\n","\n","# 1 word is not spelled correctly \"Opn\"\n","print(spelling_correction(\"Opn the garage door.\"))\n","\n","# 2 words are not spelled correctly \"youu\" \"doorr\"\n","print(spelling_correction(\"Can youu please opennn the doorr.\"))\n","\n","# 2 words are not spelled correctly \"lihts\" \"rooom\"\n","print(spelling_correction(\"Turn off the lihts in the dining rooom.\"))\n","\n","#1 word is not spelled correctly\n","print(spelling_correction(\"lihts\"))\n"]},{"cell_type":"markdown","source":["Spelling correction in given commentary input sentences with help of dataset of IPL_LIVE_HIGHLIGHTS_COMMENTARY"],"metadata":{"id":"wZG4g4efzkCJ"}},{"cell_type":"code","source":["import os\n","from nltk import word_tokenize\n","import itertools\n","import pandas as pd\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","df = pd.read_csv(\"/content/IPL_Match_Highlights_Commentary.csv\")\n","sentences_df = df[['Commentary']]\n","sentences_df.head(10)\n","\n","def get_plain_vocabluary():\n","    sentencess = [word_tokenize(sentence['Commentary']) for index, sentence in sentences_df.iterrows()]\n","    mergesentences = list(itertools.chain.from_iterable(sentencess))\n","    plainvocabulary = list(set(mergesentences))\n","    return plainvocabulary\n","\n","def levenshtein_distance(s1, s2):\n","    if len(s1) > len(s2):\n","        s1, s2 = s2, s1\n","\n","    distances = range(len(s1) + 1)\n","    for i2, c2 in enumerate(s2):\n","        distances_ = [i2+1]\n","        for i1, c1 in enumerate(s1):\n","            if c1 == c2:\n","                distances_.append(distances[i1])\n","            else:\n","                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n","        distances = distances_\n","    return distances[-1]\n","\n","def spelling_correction(sentence):\n","    splittedsentence = word_tokenize(sentence)\n","    vocwords = list(itertools.chain.from_iterable([get_plain_vocabluary()]))\n","    for i,word in enumerate(splittedsentence):\n","        if (word not in vocwords and not word.isdigit()): # ignore digits\n","            levdistances = []\n","            for vocword in vocwords:\n","                levdistances.append(levenshtein_distance(word,vocword))\n","            splittedsentence[i] = vocwords[levdistances.index(min(levdistances))]\n","        else:\n","            splittedsentence[i] = word\n","    return ' '.join(splittedsentence)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","print(spelling_correction(\"fourr\"))\n","\n","print(spelling_correction(\"Sdx\"))\n","\n","print(spelling_correction(\"Virat Koli het a masive sdx in the mid wiket\"))\n","\n","print(spelling_correction(\"MS Dhoni is bck! He's back to whet he does bst\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNxLPlDLzXFB","executionInfo":{"status":"ok","timestamp":1682670722127,"user_tz":-330,"elapsed":31815,"user":{"displayName":"TY_B_10_Jayendra Borse","userId":"08845089155460103684"}},"outputId":"8426047f-7b70-432c-db7d-3936e25acb7d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["four\n","Six\n","Virat Kohli set a massive six in the mid wicket\n","MS Dhoni is back ! He 's back to when he does bit\n"]}]},{"cell_type":"markdown","source":["Spelling correction in input of news headline in Marathi with help of dataset of Marathi news headline"],"metadata":{"id":"O7IQzEWc0ydo"}},{"cell_type":"code","source":["import os\n","from nltk import word_tokenize\n","import itertools\n","import pandas as pd\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","df = pd.read_csv(\"train.csv\")\n","sentences_df = df[['headline']]\n","sentences_df.head(10)\n","\n","def get_plain_vocabluary():\n","    sentencess = [word_tokenize(sentence['headline']) for index, sentence in sentences_df.iterrows()]\n","    mergesentences = list(itertools.chain.from_iterable(sentencess))\n","    plainvocabulary = list(set(mergesentences))\n","    return plainvocabulary\n","\n","def levenshtein_distance(s1, s2):\n","    if len(s1) > len(s2):\n","        s1, s2 = s2, s1\n","\n","    distances = range(len(s1) + 1)\n","    for i2, c2 in enumerate(s2):\n","        distances_ = [i2+1]\n","        for i1, c1 in enumerate(s1):\n","            if c1 == c2:\n","                distances_.append(distances[i1])\n","            else:\n","                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n","        distances = distances_\n","    return distances[-1]\n","\n","def spelling_correction(sentence):\n","    splittedsentence = word_tokenize(sentence)\n","    vocwords = list(itertools.chain.from_iterable([get_plain_vocabluary()]))\n","    for i,word in enumerate(splittedsentence):\n","        if (word not in vocwords and not word.isdigit()): \n","            levdistances = []\n","            for vocword in vocwords:\n","                levdistances.append(levenshtein_distance(word,vocword))\n","            splittedsentence[i] = vocwords[levdistances.index(min(levdistances))]\n","        else:\n","            splittedsentence[i] = word\n","    return ' '.join(splittedsentence)\n","\n","import nltk\n","nltk.download('punkt')\n","\n","print(spelling_correction(\"सचिन तेंडुलfefefefeकरने बाळासाहबांच्या आठवwdwddwwddwणींना \"))\n","\n","print(spelling_correction(\"एबी डिव्हिलियlkljर्सचे सुपरsdfsaफास्ट शrtggतक, अवघ्या 31 चेंडूत ठोqwrwfकले शतक\"))\n","\n","print(spelling_correction(\"मुंबghghgई\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmqRN_Hq0zph","executionInfo":{"status":"ok","timestamp":1682668303295,"user_tz":-330,"elapsed":16517,"user":{"displayName":"TY_B_10_Jayendra Borse","userId":"08845089155460103684"}},"outputId":"67fa900e-5d37-408f-a502-158fcb81c40e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["सचिन तेंडुलकरने बाळासाहेबांच्या आठवणींना\n","एबी डिव्हिलियर्सचे सुपरफास्ट शतक , अवघ्या 31 चेंडूत ठोकले शतक\n","मुंबई\n"]}]}]}