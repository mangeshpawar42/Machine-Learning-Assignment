{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19kNqMofgOVGg8GoEL4GqFKAn5gHjFyTq","timestamp":1682672515692}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVYo0gMtxXr5","executionInfo":{"status":"ok","timestamp":1683036262656,"user_tz":-330,"elapsed":15364,"user":{"displayName":"TY_B_10_Jayendra Borse","userId":"08845089155460103684"}},"outputId":"5ccaf153-5a1d-4294-98a8-3160ed7c6995"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: PyPDF2==2.12.1 in /usr/local/lib/python3.10/dist-packages (2.12.1)\n","ERROR: unknown command \"isntall\" - maybe you meant \"install\"\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n"]}],"source":["!pip install PyPDF2==2.12.1\n","!pip isntall python-dox\n","!pip install requests\n","!pip install bs4"]},{"cell_type":"code","source":["import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","import spacy\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Load the stop words\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Load the stemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Function to perform tokenization\n","def tokenize(text):\n","    tokens = word_tokenize(text)\n","    tokens = [word.lower() for word in tokens if word.isalpha()]\n","    return tokens\n","\n","# Function to perform stemming\n","def stem(tokens):\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","    return stemmed_tokens\n","\n","# Function to perform lemmatization\n","def lemmatize(tokens):\n","    doc = nlp(\" \".join(tokens))\n","    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return lemmatized_tokens\n","\n","# Function to remove stop words\n","def remove_stop_words(tokens):\n","    filtered_tokens = [word for word in tokens if not word in stop_words]\n","    return filtered_tokens\n","\n","# Load the text from the doc file\n","with open(\"document.txt\") as file:\n","    text = file.read()\n","\n","# Tokenize the text\n","tokens = tokenize(text)\n","\n","# Perform stemming\n","stemmed_tokens = stem(tokens)\n","\n","# Perform lemmatization\n","lemmatized_tokens = lemmatize(tokens)\n","\n","# Remove stop words\n","filtered_tokens = remove_stop_words(lemmatized_tokens)\n","\n","# Print the tokens\n","print(\"Original Tokens:\", tokens[:20])\n","print(\"Stemmed Tokens:\", stemmed_tokens[:20])\n","print(\"Lemmatized Tokens:\", lemmatized_tokens[:20])\n","print(\"Filtered Tokens:\", filtered_tokens[:20])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cH0VL09ozAnx","executionInfo":{"status":"ok","timestamp":1682667852711,"user_tz":-330,"elapsed":1720,"user":{"displayName":"TY_B_14_Rahul Choubey","userId":"16670125581561583365"}},"outputId":"6622e3a1-d8aa-4e9a-bd1c-6a2fd1984dd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens: ['mumbai', 'is', 'capital', 'of', 'maharashtra', 'it', 'is', 'economical', 'capital', 'of', 'india']\n","Stemmed Tokens: ['mumbai', 'is', 'capit', 'of', 'maharashtra', 'it', 'is', 'econom', 'capit', 'of', 'india']\n","Lemmatized Tokens: ['mumbai', 'capital', 'maharashtra', 'economical', 'capital', 'india']\n","Filtered Tokens: ['mumbai', 'capital', 'maharashtra', 'economical', 'capital', 'india']\n"]}]},{"cell_type":"code","source":["!pip install python-docx\n","import docx\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","import spacy\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Load the stop words\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Load the stemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Function to perform tokenization\n","def tokenize(text):\n","    tokens = word_tokenize(text)\n","    tokens = [word.lower() for word in tokens if word.isalpha()]\n","    return tokens\n","\n","# Function to perform stemming\n","def stem(tokens):\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","    return stemmed_tokens\n","\n","# Function to perform lemmatization\n","def lemmatize(tokens):\n","    doc = nlp(\" \".join(tokens))\n","    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return lemmatized_tokens\n","\n","# Function to remove stop words\n","def remove_stop_words(tokens):\n","    filtered_tokens = [word for word in tokens if not word in stop_words]\n","    return filtered_tokens\n","\n","# load the document\n","doc = docx.Document('Document.docx')\n","\n","# extract the content\n","text = ''\n","for paragraph in doc.paragraphs:\n","    text += paragraph.text\n","\n","# Tokenize the text\n","tokens = tokenize(text)\n","\n","# Perform stemming\n","stemmed_tokens = stem(tokens)\n","\n","# Perform lemmatization\n","lemmatized_tokens = lemmatize(tokens)\n","\n","# Remove stop words\n","filtered_tokens = remove_stop_words(lemmatized_tokens)\n","\n","# Print the tokens\n","print(\"Original Tokens:\", tokens[:20])\n","print(\"Stemmed Tokens:\", stemmed_tokens[:20])\n","print(\"Lemmatized Tokens:\", lemmatized_tokens[:20])\n","print(\"Filtered Tokens:\", filtered_tokens[:20])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDRk3W821Ssr","executionInfo":{"status":"ok","timestamp":1682668450295,"user_tz":-330,"elapsed":5693,"user":{"displayName":"TY_B_14_Rahul Choubey","userId":"16670125581561583365"}},"outputId":"4a5198f5-599c-4fe6-ec08-68ec79c3bd8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (0.8.11)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens: ['delhi', 'is', 'capital', 'of', 'india']\n","Stemmed Tokens: ['delhi', 'is', 'capit', 'of', 'india']\n","Lemmatized Tokens: ['delhi', 'capital', 'india']\n","Filtered Tokens: ['delhi', 'capital', 'india']\n"]}]},{"cell_type":"code","source":["!pip install PyPDF2\n","import PyPDF2\n","import docx\n","import nltk\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","import spacy\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Load the stop words\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Load the stemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Function to perform tokenization\n","def tokenize(text):\n","    tokens = word_tokenize(text)\n","    tokens = [word.lower() for word in tokens if word.isalpha()]\n","    return tokens\n","\n","# Function to perform stemming\n","def stem(tokens):\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","    return stemmed_tokens\n","\n","# Function to perform lemmatization\n","def lemmatize(tokens):\n","    doc = nlp(\" \".join(tokens))\n","    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return lemmatized_tokens\n","\n","# Function to remove stop words\n","def remove_stop_words(tokens):\n","    filtered_tokens = [word for word in tokens if not word in stop_words]\n","    return filtered_tokens\n","\n","# open the PDF file in read binary mode\n","pdf_file = open('Document.pdf', 'rb')\n","\n","# create a PDF reader object\n","pdf_reader = PyPDF2.PdfReader(pdf_file)\n","\n","# get the number of pages\n","num_pages = len(pdf_reader.pages)\n","#pdf_reader.numPages\n","\n","# initialize an empty string to store the text\n","text = ''\n","\n","# loop through each page and extract the text\n","for i in range(num_pages):\n","    page = pdf_reader.pages[i]\n","    #pdf_reader.getPage(i)\n","    text += page.extract_text()\n","    #page.extractText()\n","\n","# Tokenize the text\n","tokens = tokenize(text)\n","\n","# Perform stemming\n","stemmed_tokens = stem(tokens)\n","\n","# Perform lemmatization\n","lemmatized_tokens = lemmatize(tokens)\n","\n","# Remove stop words\n","filtered_tokens = remove_stop_words(lemmatized_tokens)\n","\n","# Print the tokens\n","print(\"Original Tokens:\", tokens[:20])\n","print(\"Stemmed Tokens:\", stemmed_tokens[:20])\n","print(\"Lemmatized Tokens:\", lemmatized_tokens[:20])\n","print(\"Filtered Tokens:\", filtered_tokens[:20])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPDaTteG3YIR","executionInfo":{"status":"ok","timestamp":1682668786145,"user_tz":-330,"elapsed":7360,"user":{"displayName":"TY_B_14_Rahul Choubey","userId":"16670125581561583365"}},"outputId":"85f1b57b-a5fc-44d8-adc2-ff7036324eea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (2.12.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens: ['delhi', 'is', 'capital', 'of', 'india']\n","Stemmed Tokens: ['delhi', 'is', 'capit', 'of', 'india']\n","Lemmatized Tokens: ['delhi', 'capital', 'india']\n","Filtered Tokens: ['delhi', 'capital', 'india']\n"]}]},{"cell_type":"code","source":["!pip install beautifulsoup4\n","import requests\n","import nltk\n","import string\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","import spacy\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Load the stop words\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Load the stemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# Function to perform tokenization\n","def tokenize(text):\n","    tokens = word_tokenize(text)\n","    tokens = [word.lower() for word in tokens if word.isalpha()]\n","    return tokens\n","\n","# Function to perform stemming\n","def stem(tokens):\n","    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","    return stemmed_tokens\n","\n","# Function to perform lemmatization\n","def lemmatize(tokens):\n","    doc = nlp(\" \".join(tokens))\n","    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n","    return lemmatized_tokens\n","\n","# Function to remove stop words\n","def remove_stop_words(tokens):\n","    filtered_tokens = [word for word in tokens if not word in stop_words]\n","    return filtered_tokens\n","\n","# get the website content\n","url = 'https://www.google.com'\n","response = requests.get(url)\n","soup = BeautifulSoup(response.content, 'html.parser')\n","text = soup.get_text()\n","\n","# Tokenize the text\n","tokens = tokenize(text)\n","\n","# Perform stemming\n","stemmed_tokens = stem(tokens)\n","\n","# Perform lemmatization\n","lemmatized_tokens = lemmatize(tokens)\n","\n","# Remove stop words\n","filtered_tokens = remove_stop_words(lemmatized_tokens)\n","\n","# Print the tokens\n","print(\"Original Tokens:\", tokens[:20])\n","print(\"Stemmed Tokens:\", stemmed_tokens[:20])\n","print(\"Lemmatized Tokens:\", lemmatized_tokens[:20])\n","print(\"Filtered Tokens:\", filtered_tokens[:20])\n"],"metadata":{"id":"lqu0qczZ4MoX","executionInfo":{"status":"ok","timestamp":1682668848174,"user_tz":-330,"elapsed":5759,"user":{"displayName":"TY_B_14_Rahul Choubey","userId":"16670125581561583365"}},"outputId":"ab4842ac-a460-4e5f-fcd3-cb551667feed","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Original Tokens: ['googlesearch', 'images', 'maps', 'play', 'youtube', 'news', 'gmail', 'drive', 'more', 'web', 'history', 'settings', 'sign', 'in', 'advanced', 'searchmeet', 'social', 'impact', 'leaders', 'tackling']\n","Stemmed Tokens: ['googlesearch', 'imag', 'map', 'play', 'youtub', 'news', 'gmail', 'drive', 'more', 'web', 'histori', 'set', 'sign', 'in', 'advanc', 'searchmeet', 'social', 'impact', 'leader', 'tackl']\n","Lemmatized Tokens: ['googlesearch', 'image', 'map', 'play', 'youtube', 'news', 'gmail', 'drive', 'web', 'history', 'setting', 'sign', 'advanced', 'searchmeet', 'social', 'impact', 'leader', 'tackle', 'world', 'big']\n","Filtered Tokens: ['googlesearch', 'image', 'map', 'play', 'youtube', 'news', 'gmail', 'drive', 'web', 'history', 'setting', 'sign', 'advanced', 'searchmeet', 'social', 'impact', 'leader', 'tackle', 'world', 'big']\n"]}]}]}